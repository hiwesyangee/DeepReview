2.1 Spark调优之开发调优

Spark性能优化的第一步，就是要在开发Spark作业的过程中注意和应用一些性能优化的基本原则。
开发调优，就是要让大家了解以下一些Spark基本开发原则，包括：RDD lineage设计、算子的合理使用、特殊操作的优化等。
在开发过程中，时时刻刻都应该注意以上原则，并将这些原则根据具体的业务以及实际的应用场景，灵活地运用到自己的Spark作业中。

原则一：避免创建重复的RDD
    对于同一份数据，只应该创建一个RDD，不能创建多个RDD来代表同一份数据。

    //也就是说，需要对一份数据执行两次算子操作。
    //错误的做法：对于同一份数据执行多次算子操作时，创建多个RDD。
    //这里执行了两次textFile方法，针对同一个HDFS文件，创建了两个RDD出来，
    //然后分别对每个RDD都执行了一个算子操作。
    //这种情况下，Spark需要从HDFS上两次加载hello.txt文件的内容，并创建两个单独的RDD；
    //第二次加载HDFS文件以及创建RDD的性能开销，很明显是白白浪费掉的。

    val rdd1 = sc.textFile("hdfs://192.168.0.1:9000/hello.txt")
    rdd1.map(...)
    val rdd2 = sc.textFile("hdfs://192.168.0.1:9000/hello.txt")
    rdd2.reduce(...)

    //正确的用法：对于一份数据执行多次算子操作时，只使用一个RDD。
    //这种写法很明显比上一种写法要好多了，因为我们对于同一份数据只创建了一个RDD，
    //然后对这一个RDD执行了多次算子操作。
    //但是要注意到这里为止优化还没有结束，由于rdd1被执行了两次算子操作，第二次执行reduce操作的时候，
    //还会再次从源头处重新计算一次rdd1的数据，因此还是会有重复计算的性能开销。
    //要彻底解决这个问题，必须结合“原则三：对多次使用的RDD进行持久化”，
    //才能保证一个RDD被多次使用时只被计算一次。

    val rdd1 = sc.textFile("hdfs://hiwes:9000/hello.txt")
    rdd1.map(...)

原则二：尽可能复用同一个RDD
     除了要避免在开发过程中对一份完全相同的数据创建多个RDD之外，在对不同的数据执行算子操作时还要尽可能地复用一个RDD。
     比如说，有一个RDD的数据格式是key-value类型的，另一个是单value类型的，这两个RDD的value数据是完全一样的。
     那么此时我们可以只使用key-value类型的那个RDD，因为其中已经包含了另一个的数据。
     对于类似这种多个RDD的数据有重叠或者包含的情况，我们应该尽量复用一个RDD，这样可以尽可能地减少RDD的数量，
     从而尽可能减少算子执行的次数。

     // 错误的做法。
     // 有一个<Long, String>格式的RDD，即rdd1。
     // 接着由于业务需要，对rdd1执行了一个map操作，创建了一个rdd2，而rdd2中的数据仅仅是rdd1中的value值而已，也就是说，rdd2是rdd1的子集。

     JavaPairRDD<Long, String> rdd1 = ...
     JavaRDD<String> rdd2 = rdd1.map(...)
     // 分别对rdd1和rdd2执行了不同的算子操作。
     rdd1.reduceByKey(...)
     rdd2.map(...)

     // 正确的做法。
     // 上面这个case中，其实rdd1和rdd2的区别无非就是数据格式不同而已，rdd2的数据完全就是rdd1的子集而已，却创建了两个rdd，并对两个rdd都执行了一次算子操作。
     // 此时会因为对rdd1执行map算子来创建rdd2，而多执行一次算子操作，进而增加性能开销。
     // 其实在这种情况下完全可以复用同一个RDD。
     // 我们可以使用rdd1，既做reduceByKey操作，也做map操作。
     // 在进行第二个map操作时，只使用每个数据的tuple._2，也就是rdd1中的value值，即可。

     JavaPairRDD<Long, String> rdd1 = ...
     rdd1.reduceByKey(...)
     rdd1.map(tuple._2...)

     // 第二种方式相较于第一种方式而言，很明显减少了一次rdd2的计算开销。
     // 但是到这里为止，优化还没有结束，对rdd1我们还是执行了两次算子操作，rdd1实际上还是会被计算两次。
     // 因此还需要配合“原则三：对多次使用的RDD进行持久化”进行使用，才能保证一个RDD被多次使用时只被计算一次。

原则三：对多次使用的RDD进行持久化
        Spark中对于一个RDD执行多次算子的默认原理是这样的：每次你对一个RDD执行一个算子操作时，
        都会重新从源头处计算一遍，计算出那个RDD来，然后再对这个RDD执行你的算子操作。这种方式的性能是很差的。
        因此对于这种情况，建议是：对多次使用的RDD进行持久化。此时Spark就会根据你的持久化策略，将RDD中的数据
        保存到内存或者磁盘中。以后每次对这个RDD进行算子操作时，都会直接从内存或磁盘中提取持久化的RDD数据，
        然后执行算子，而不会从源头处重新计算一遍这个RDD，再执行算子操作。

        val rdd1 = sc.textFile("hdfs://192.168.0.1:9000/hello.txt").persist(StorageLevel.MEMORY_AND_DISK_SER)
        rdd1.map(...)
        rdd1.reduce(...)

 Spark的持久化级别
        持久化级别	                            含义解释
        MEMORY_ONLY	            使用未序列化的Java对象格式，将数据保存在内存中。如果内存不够存放所有的数据，
                                则数据可能就不会进行持久化。那么下次对这个RDD执行算子操作时，那些没有被持久化的数据，
                                需要从源头处重新计算一遍。这是默认的持久化策略，使用cache()方法时，
                                实际就是使用的这种持久化策略。
        MEMORY_AND_DISK	        使用未序列化的Java对象格式，优先尝试将数据保存在内存中。如果内存不够存放所有的数据，
                                会将数据写入磁盘文件中，下次对这个RDD执行算子时，持久化在磁盘文件中的数据会被读取出来使用。
        MEMORY_ONLY_SER	        基本含义同MEMORY_ONLY。唯一的区别是，会将RDD中的数据进行序列化，RDD的每个partition
                                会被序列化成一个字节数组。这种方式更加节省内存，从而可以避免持久化的数据占用过多内存导致频繁GC。
        MEMORY_AND_DISK_SER	    基本含义同MEMORY_AND_DISK。唯一的区别是，会将RDD中的数据进行序列化，RDD的每个
                                partition会被序列化成一个字节数组。这种方式更加节省内存，从而可以避免持久化的数据
                                占用过多内存导致频繁GC。
        DISK_ONLY	            使用未序列化的Java对象格式，将数据全部写入磁盘文件中。
        MEMORY_ONLY_2, MEMORY_AND_DISK_2, 等等.	对于上述任意一种持久化策略，如果加上后缀_2，代表的是将每个持久化的
                                数据，都复制一份副本，并将副本保存到其他节点上。这种基于副本的持久化机制主要用于进行容错。
                                假如某个节点挂掉，节点的内存或磁盘中的持久化数据丢失了，那么后续对RDD计算时还可以使用该
                                数据在其他节点上的副本。如果没有副本的话，就只能将这些数据从源头处重新计算一遍了。

 如何选择一种最合适的持久化策略
        默认情况下，性能最高的当然是MEMORY_ONLY，但前提是你的内存必须足够足够大，可以绰绰有余地存放下整个RDD的所有数据。
        因为不进行序列化与反序列化操作，就避免了这部分的性能开销；对这个RDD的后续算子操作，都是基于纯内存中的数据的操作，
        不需要从磁盘文件中读取数据，性能也很高；而且不需要复制一份数据副本，并远程传送到其他节点上。但是这里必须要注意的是，
        在实际的生产环境中，恐怕能够直接用这种策略的场景还是有限的，如果RDD中数据比较多时（比如几十亿），直接用这种
        持久化级别，会导致JVM的OOM内存溢出异常。

        如果使用MEMORY_ONLY级别时发生了内存溢出，那么建议尝试使用MEMORY_ONLY_SER级别。该级别会将RDD数据序列化后再保存
        在内存中，此时每个partition仅仅是一个字节数组而已，大大减少了对象数量，并降低了内存占用。这种级别比MEMORY_ONLY
        多出来的性能开销，主要就是序列化与反序列化的开销。但是后续算子可以基于纯内存进行操作，因此性能总体还是比较高的。
        此外，可能发生的问题同上，如果RDD中的数据量过多的话，还是可能会导致OOM内存溢出的异常。

        如果纯内存的级别都无法使用，那么建议使用MEMORY_AND_DISK_SER策略，而不是MEMORY_AND_DISK策略。因为既然到了这一步，
        就说明RDD的数据量很大，内存无法完全放下。序列化后的数据比较少，可以节省内存和磁盘的空间开销。同时该策略会优先尽量尝试
        将数据缓存在内存中，内存缓存不下才会写入磁盘。

        通常不建议使用DISK_ONLY和后缀为_2的级别：因为完全基于磁盘文件进行数据的读写，会导致性能急剧降低，有时还不如重新计算
        一次所有RDD。后缀为_2的级别，必须将所有数据都复制一份副本，并发送到其他节点上，数据复制以及网络传输会导致较大的性能开销，
        除非是要求作业的高可用性，否则不建议使用。

原则四：尽量避免使用shuffle类算子
        shuffle过程中，各个节点上的相同key都会先写入本地磁盘文件中，然后其他节点需要通过网络传输拉取各个节点上的磁盘文件
        中的相同key。而且相同key都拉取到同一个节点进行聚合操作时，还有可能会因为一个节点上处理的key过多，导致内存不够存放，
        进而溢写到磁盘文件中。因此在shuffle过程中，可能会发生大量的磁盘文件读写的IO操作，以及数据的网络传输操作。
        磁盘IO和网络数据传输也是shuffle性能较差的主要原因。

        因此在开发过程中，能避免则尽可能避免使用reduceByKey、join、distinct、repartition等会进行shuffle的算子，
        尽量使用map类的非shuffle算子。这样的话，没有shuffle操作或者仅有较少shuffle操作的Spark作业，可以大大减少性能开销。

原则五：使用map-side预聚合的shuffle操作
        如果因为业务需要，一定要使用shuffle操作，无法用map类的算子来替代，那么尽量使用可以map-side预聚合的算子。
        所谓的map-side预聚合，说的是在每个节点本地对相同的key进行一次聚合操作，类似于MapReduce中的本地combiner。
        map-side预聚合之后，每个节点本地就只会有一条相同的key，因为多条相同的key都被聚合起来了。其他节点在拉取所有
        节点上的相同key时，就会大大减少需要拉取的数据数量，从而也就减少了磁盘IO以及网络传输开销。通常来说，在可能的情况下，
        建议使用reduceByKey或者aggregateByKey算子来替代掉groupByKey算子。因为reduceByKey和aggregateByKey算子
        都会使用用户自定义的函数对每个节点本地的相同key进行预聚合。而groupByKey算子是不会进行预聚合的，全量的数据会在
        集群的各个节点之间分发和传输，性能相对来说比较差。

原则六：使用高性能的算子
    除了shuffle相关的算子有优化原则之外，其他的算子也都有着相应的优化原则。
        使用reduceByKey/aggregateByKey替代groupByKey
        使用mapPartitions替代普通map
            mapPartitions类的算子，一次函数调用会处理一个partition所有的数据，而不是一次函数调用处理一条，
            性能相对来说会高一些。但是有的时候，使用mapPartitions会出现OOM（内存溢出）的问题。
            因为单次函数调用就要处理掉一个partition所有的数据，如果内存不够，垃圾回收时是无法回收掉太多对象的，
            很可能出现OOM异常。所以使用这类操作时要慎重！
        使用foreachPartitions替代foreach
            原理类似于“使用mapPartitions替代map”，也是一次函数调用处理一个partition的所有数据，而不是一次
            函数调用处理一条数据。在实践中发现，foreachPartitions类的算子，对性能的提升还是很有帮助的。
            比如在foreach函数中，将RDD中所有数据写MySQL，那么如果是普通的foreach算子，就会一条数据一条数据地写，
            每次函数调用可能就会创建一个数据库连接，此时就势必会频繁地创建和销毁数据库连接，性能是非常低下；
            但是如果用foreachPartitions算子一次性处理一个partition的数据，那么对于每个partition，只要创建一个
            数据库连接即可，然后执行批量插入操作，此时性能是比较高的。实践中发现，对于1万条左右的数据量写MySQL，
            性能可以提升30%以上。
        使用filter之后进行coalesce操作
            通常对一个RDD执行filter算子过滤掉RDD中较多数据后（比如30%以上的数据），建议使用coalesce算子，
            手动减少RDD的partition数量，将RDD中的数据压缩到更少的partition中去。因为filter之后，RDD的每个partition
            中都会有很多数据被过滤掉，此时如果照常进行后续的计算，其实每个task处理的partition中的数据量并不是很多，
            有一点资源浪费，而且此时处理的task越多，可能速度反而越慢。因此用coalesce减少partition数量，将RDD中的数据
            压缩到更少的partition之后，只要使用更少的task即可处理完所有的partition。在某些场景下，对于性能的提升会有
            一定的帮助。
        使用repartitionAndSortWithinPartitions替代repartition与sort类操作
            repartitionAndSortWithinPartitions是Spark官网推荐的一个算子，官方建议，如果需要在repartition重分区之后，
            还要进行排序，建议直接使用repartitionAndSortWithinPartitions算子。因为该算子可以一边进行重分区的shuffle
            操作，一边进行排序。shuffle与sort两个操作同时进行，比先shuffle再sort来说，性能可能是要高的。

原则七：广播大变量
      有时在开发过程中，会遇到需要在算子函数中使用外部变量的场景（尤其是大变量，比如100M以上的大集合），那么此时就应该
      使用Spark的广播（Broadcast）功能来提升性能。
      在算子函数中使用到外部变量时，默认情况下，Spark会将该变量复制多个副本，通过网络传输到task中，此时每个task都有
      一个变量副本。如果变量本身比较大的话（比如100M，甚至1G），那么大量的变量副本在网络中传输的性能开销，以及在各个节点
      的Executor中占用过多内存导致的频繁GC，都会极大地影响性能。
      因此对于上述情况，如果使用的外部变量比较大，建议使用Spark的广播功能，对该变量进行广播。广播后的变量，会保证每个
      Executor的内存中，只驻留一份变量副本，而Executor中的task执行时共享该Executor中的那份变量副本。这样的话，
      可以大大减少变量副本的数量，从而减少网络传输的性能开销，并减少对Executor内存的占用开销，降低GC的频率。

      // 以下代码在算子函数中，使用了外部的变量。
      // 此时没有做任何特殊操作，每个task都会有一份list1的副本。
      val list1 = ...
      rdd1.map(list1...)

      // 以下代码将list1封装成了Broadcast类型的广播变量。
      // 在算子函数中，使用广播变量时，首先会判断当前task所在Executor内存中，是否有变量副本。
      // 如果有则直接使用；如果没有则从Driver或者其他Executor节点上远程拉取一份放到本地Executor内存中。
      // 每个Executor内存中，就只会驻留一份广播变量副本。
      val list1 = ...
      val list1Broadcast = sc.broadcast(list1)
      rdd1.map(list1Broadcast...)

原则八：使用Kryo优化序列化性能

      在Spark中，主要有三个地方涉及到了序列化：
        1) 在算子函数中使用到外部变量时，该变量会被序列化后进行网络传输（见“原则七：广播大变量”中的讲解）。
        2) 将自定义的类型作为RDD的泛型类型时（比如JavaRDD，Student是自定义类型），所有自定义类型对象，都会进行序列化。
            因此这种情况下，也要求自定义的类必须实现Serializable接口。
        3) 使用可序列化的持久化策略时（比如MEMORY_ONLY_SER），Spark会将RDD中的每个partition都序列化成一个大的字节数组。
      对于这三种出现序列化的地方，我们都可以通过使用Kryo序列化类库，来优化序列化和反序列化的性能。
      Spark默认使用的是Java的序列化机制，也就是ObjectOutputStream/ObjectInputStream API来进行序列化和反序列化。
      但是Spark同时支持使用Kryo序列化库，Kryo序列化类库的性能比Java序列化类库的性能要高很多。
      官方介绍，Kryo序列化机制比Java序列化机制，性能高10倍左右。Spark之所以默认没有使用Kryo作为序列化类库，
            是因为Kryo要求最好要注册所有需要进行序列化的自定义类型，因此对于开发者来说，这种方式比较麻烦。

      以下是使用Kryo的代码示例，只要设置序列化类，再注册要序列化的自定义类型即可（比如算子函数中使用到的外部变量类型、
      作为RDD泛型类型的自定义类型等）：
      // 创建SparkConf对象。
      val conf = new SparkConf().setMaster(...).setAppName(...)
      // 设置序列化器为KryoSerializer。
      conf.set("spark.serializer", "org.apache.spark.serializer.KryoSerializer")
      // 注册要序列化的自定义类型。
      conf.registerKryoClasses(Array(classOf[MyClass1], classOf[MyClass2]))

原则九：优化数据结构
      Java中，有三种类型比较耗费内存：
        1）对象，每个Java对象都有对象头、引用等额外的信息，因此比较占用内存空间。
        2）字符串，每个字符串内部都有一个字符数组以及长度等额外信息。
        3）集合类型，比如HashMap、LinkedList等，因为集合类型内部通常会使用一些内部类来封装集合元素，比如Map.Entry。
      因此Spark官方建议，在Spark编码实现中，特别是对于算子函数中的代码，尽量不要使用上述三种数据结构，
      尽量使用字符串替代对象，使用原始类型（比如Int、Long）替代字符串，使用数组替代集合类型，这样尽可能地减少内存占用，
      从而降低GC频率，提升性能。

      但是在实践中要做到该原则其实并不容易。因为同时要考虑到代码的可维护性，如果一个代码中，完全没有任何对象抽象，
      全部是字符串拼接的方式，那么对于后续的代码维护和修改，无疑是一场巨大的灾难。
      同理，如果所有操作都基于数组实现，而不使用HashMap、LinkedList等集合类型，那么对于编码难度以及代码可维护性，
      也是一个极大的挑战。因此建议，在可能以及合适的情况下，使用占用内存较少的数据结构，但是前提是要保证代码的可维护性。